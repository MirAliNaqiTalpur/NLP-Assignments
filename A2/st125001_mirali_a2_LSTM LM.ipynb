{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - AG News\n",
    "\n",
    "The dataset is called AG's News Topic Classification Dataset. It consists of over 1 million news articles categorized into 4 classes: World, Sports, Business, and Sci/Tech. The dataset is used as a benchmark for text classification tasks and was originally gathered from over 2000 news sources. It includes both a training set (120,000 samples) and a testing set (7,600 samples), with 30,000 training samples and 1,900 testing samples per class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AG's News Topic Classification Dataset** \n",
    "\n",
    "- **Source**: The AG's News Topic Classification Dataset contains over 1 million news articles, categorized into 4 classes: World, Sports, Business, and Sci/Tech. It was created by **ComeToMyHead**, an academic news search engine.\n",
    "  \n",
    "- **Description**: The dataset includes 120,000 training samples and 7,600 testing samples. It is commonly used for text classification benchmarks.\n",
    "\n",
    "- **Citation**:  \n",
    "  Zhang, X., Zhao, J., & LeCun, Y. (2015). \"Character-level Convolutional Networks for Text Classification,\" *Advances in Neural Information Processing Systems 28 (NIPS 2015)*.\n",
    "\n",
    "- **Dataset Links**:  \n",
    "  - [Original Dataset](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)  \n",
    "  - [Hugging Face Dataset](https://huggingface.co/datasets/ag_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('ag_news')\n",
    "# Remove 'label' column from both the train and test datasets\n",
    "dataset = dataset.remove_columns(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic Material May Help Make Nano-Devices: Study (Reuters) Reuters - The genetic building blocks that\\form the basis for life may also be used to build the tiny\\machines of nanotechnology, U.S. researchers said on Thursday.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][550]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Using custom functions to tokenize, since torchtext is not comapatible with the current version of pytorch. It was decided to use custom function for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple whitespace-based tokenizer (no subword tokenization)\n",
    "def simple_tokenize(texts):\n",
    "    # Tokenize each text in the batch\n",
    "    return [text.split() for text in texts]\n",
    "\n",
    "# Tokenize data using simple whitespace tokenization (for batched processing)\n",
    "def tokenize_data(batch):\n",
    "    return {'tokens': simple_tokenize(batch['text'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 tokenized train examples: [['Wall', 'St.', 'Bears', 'Claw', 'Back', 'Into', 'the', 'Black', '(Reuters)', 'Reuters', '-', 'Short-sellers,', 'Wall', \"Street's\", 'dwindling\\\\band', 'of', 'ultra-cynics,', 'are', 'seeing', 'green', 'again.'], ['Carlyle', 'Looks', 'Toward', 'Commercial', 'Aerospace', '(Reuters)', 'Reuters', '-', 'Private', 'investment', 'firm', 'Carlyle', 'Group,\\\\which', 'has', 'a', 'reputation', 'for', 'making', 'well-timed', 'and', 'occasionally\\\\controversial', 'plays', 'in', 'the', 'defense', 'industry,', 'has', 'quietly', 'placed\\\\its', 'bets', 'on', 'another', 'part', 'of', 'the', 'market.'], ['Oil', 'and', 'Economy', 'Cloud', \"Stocks'\", 'Outlook', '(Reuters)', 'Reuters', '-', 'Soaring', 'crude', 'prices', 'plus', 'worries\\\\about', 'the', 'economy', 'and', 'the', 'outlook', 'for', 'earnings', 'are', 'expected', 'to\\\\hang', 'over', 'the', 'stock', 'market', 'next', 'week', 'during', 'the', 'depth', 'of', 'the\\\\summer', 'doldrums.']]\n",
      "First 3 tokenized validation examples: [['Stocks', 'Rise', 'on', 'IBM', 'Earnings', 'and', 'Lower', 'Oil', 'NEW', 'YORK', '(Reuters)', '-', 'U.S.', 'stocks', 'rose', 'on', 'Tuesday', 'as', 'oil', 'prices', 'declined', 'and', 'investors', 'were', 'encouraged', 'by', \"Monday's\", 'stronger-than-expected', 'earnings', 'from', 'technology', 'bellwethers', 'International', 'Business', 'Machines', 'Corp.', '&lt;A', 'HREF=\"http://www.investor.reuters.com/FullQuote.aspx?ticker=IBM.N', 'target=/stocks/quickinfo/fullquote\"&gt;IBM.N&lt;/A&gt;', 'and', 'Texas', 'Instruments', 'Inc.', '&lt;A', 'HREF=\"http://www.investor.reuters.com/FullQuote.aspx?ticker=TXN.N', 'target=/stocks/quickinfo/fullquote\"&gt;TXN.N&lt;/A&gt;'], ['Israel', 'Kills', 'Top', 'Hamas', 'Militant', 'Ahead', 'of', 'Gaza', 'Vote', 'GAZA', '(Reuters)', '-', 'Israel', 'killed', 'the', 'top', 'bomb', 'maker', 'of', 'the', 'Hamas', 'Islamic', 'militant', 'group', 'in', 'a', 'Gaza', 'Strip', 'air', 'strike', 'on', 'Thursday,', 'days', 'before', 'a', 'key', 'parliamentary', 'vote', 'on', 'Prime', 'Minister', 'Ariel', \"Sharon's\", 'plan', 'to', 'quit', 'the', 'occupied', 'territory.'], ['Charity', 'boss', 'kidnapped', 'in', 'Iraq', 'A', 'director', 'of', 'an', 'international', 'charity', 'in', 'Iraq', 'has', 'been', 'kidnapped', 'as', 'she', 'was', 'being', 'driven', 'to', 'work', 'in', 'Baghdad.', 'Margaret', 'Hassan,', 'a', 'British-born', 'Iraqi', 'national,', 'was', 'abducted', 'in', 'the', 'capital,', 'CARE', 'International', 'UK', 'said', 'in', 'London.']]\n",
      "First 3 tokenized test examples: [['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', 'Newall', 'say', 'they', 'are', \"'disappointed'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul.'], ['The', 'Race', 'is', 'On:', 'Second', 'Private', 'Team', 'Sets', 'Launch', 'Date', 'for', 'Human', 'Spaceflight', '(SPACE.com)', 'SPACE.com', '-', 'TORONTO,', 'Canada', '--', 'A', 'second\\\\team', 'of', 'rocketeers', 'competing', 'for', 'the', '#36;10', 'million', 'Ansari', 'X', 'Prize,', 'a', 'contest', 'for\\\\privately', 'funded', 'suborbital', 'space', 'flight,', 'has', 'officially', 'announced', 'the', 'first\\\\launch', 'date', 'for', 'its', 'manned', 'rocket.'], ['Ky.', 'Company', 'Wins', 'Grant', 'to', 'Study', 'Peptides', '(AP)', 'AP', '-', 'A', 'company', 'founded', 'by', 'a', 'chemistry', 'researcher', 'at', 'the', 'University', 'of', 'Louisville', 'won', 'a', 'grant', 'to', 'develop', 'a', 'method', 'of', 'producing', 'better', 'peptides,', 'which', 'are', 'short', 'chains', 'of', 'amino', 'acids,', 'the', 'building', 'blocks', 'of', 'proteins.']]\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenizer to the entire training dataset\n",
    "tokenized_train = dataset['train'].map(tokenize_data, remove_columns=['text'], batched=True)\n",
    "\n",
    "# Apply tokenizer to the entire test dataset\n",
    "tokenized_test = dataset['test'].map(tokenize_data, remove_columns=['text'], batched=True)\n",
    "\n",
    "# Manually create the validation set from the training set (10% of the training data)\n",
    "tokenized_valid = tokenized_train.train_test_split(test_size=0.1)['test']\n",
    "\n",
    "\n",
    "# Verify the tokenization\n",
    "print(f\"First 3 tokenized train examples: {tokenized_train['tokens'][:3]}\")\n",
    "print(f\"First 3 tokenized validation examples: {tokenized_valid['tokens'][:3]}\")\n",
    "print(f\"First 3 tokenized test examples: {tokenized_test['tokens'][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7600, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Genetic', 'Material', 'May', 'Help', 'Make', 'Nano-Devices:', 'Study', '(Reuters)', 'Reuters', '-', 'The', 'genetic', 'building', 'blocks', 'that\\\\form', 'the', 'basis', 'for', 'life', 'may', 'also', 'be', 'used', 'to', 'build', 'the', 'tiny\\\\machines', 'of', 'nanotechnology,', 'U.S.', 'researchers', 'said', 'on', 'Thursday.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[550]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Second', 'Prisoner', 'Abuse', 'Report', 'Expected', 'WASHINGTON', '-', 'Inattention', 'to', 'prisoner', 'issues', 'by', 'senior', 'U.S.', 'military', 'leaders', 'in', 'Iraq', 'and', 'at', 'the', 'Pentagon', 'was', 'a', 'key', 'factor', 'in', 'the', 'abuse', 'scandal', 'at', 'Abu', 'Ghraib', 'prison,', 'but', 'there', 'is', 'no', 'evidence', 'they', 'ordered', 'any', 'mistreatment,', 'an', 'independent', 'panel', 'concluded...']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_test[550]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STANFORD', 'NOTEBOOK', 'Just', 'go', 'away', 'quietly?', 'Not', 'Stanford', '#39;s', 'angry', '&lt;b&gt;...&lt;/b&gt;', 'Guard', 'Ismail', 'Simpson', 'of', 'Stanford', 'was', 'walking', 'off', 'the', 'field', 'at', 'the', 'end', 'of', 'the', 'game,', 'yelling', 'and', 'gesturing.', 'An', 'assistant', 'coach', 'had', 'an', 'arm', 'around', 'him,', 'pushing', 'him', 'in', 'the', 'direction', 'of', 'the', 'end', 'zone', 'tunnel', 'that', 'leads', 'to', 'the', 'locker', 'room.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_valid[550]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least ten times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of tokens from the tokenized datasets (train, validation, and test)\n",
    "flat_tokens = [token for dataset in [tokenized_train, tokenized_valid, tokenized_test]\n",
    "               for tokens in dataset['tokens'] for token in tokens]\n",
    "\n",
    "# Count the frequency of each token\n",
    "token_counts = Counter(flat_tokens)\n",
    "\n",
    "# Filter tokens by frequency (min_freq = 10)\n",
    "filtered_tokens = [token for token, count in token_counts.items() if count >= 10]\n",
    "\n",
    "# Add special tokens <unk> and <eos>\n",
    "special_tokens = ['<unk>', '<eos>']\n",
    "vocab_tokens = special_tokens + filtered_tokens\n",
    "\n",
    "# Create vocabulary (mapping tokens to indices)\n",
    "vocab = {token: idx for idx, token in enumerate(vocab_tokens)}\n",
    "\n",
    "# Invert the vocab dictionary to get index-to-token mapping\n",
    "itos_vocab = {idx: token for token, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 31866\n",
      "Some sample tokens: ['<unk>', '<eos>', 'Wall', 'St.', 'Bears', 'Back', 'Into', 'the', 'Black', '(Reuters)']\n"
     ]
    }
   ],
   "source": [
    "# Print the vocabulary size and some sample tokens\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Some sample tokens: {list(vocab.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            # Append <eos> to the token list (correctly modify tokens)\n",
    "            tokens = example['tokens'] + ['<eos>']              \n",
    "            # Convert tokens to indices using vocab, using vocab.get() to handle OOV tokens\n",
    "            tokens = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "            # Extend the data with token indices\n",
    "            data.extend(tokens)\n",
    "            \n",
    "    # Convert to a tensor\n",
    "    data = torch.LongTensor(data).to(device)\n",
    "    \n",
    "    # Calculate the number of batches\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    \n",
    "    # Trim data to fit into complete batches\n",
    "    data = data[:num_batches * batch_size]\n",
    "    \n",
    "    # Reshape data to [batch_size, seq_len]\n",
    "    data = data.view(batch_size, num_batches)\n",
    "    \n",
    "    return data  # [batch_size, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_train, vocab, batch_size)\n",
    "valid_data = get_data(tokenized_valid, vocab, batch_size)\n",
    "test_data  = get_data(tokenized_test,  vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 36419])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3645])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2299])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based Language Model\n",
    "\n",
    "This class defines an LSTM-based language model using PyTorch's `nn.Module`. It includes the following key components:\n",
    "\n",
    "- **Embedding Layer**: Converts input tokens into dense vectors of a specified size (`emb_dim`).\n",
    "- **LSTM Layer**: Processes the embedded input sequence with multiple layers, hidden state dimensions (`hid_dim`), and a dropout rate for regularization.\n",
    "- **Dropout Layer**: Applied after LSTM to prevent overfitting.\n",
    "- **Fully Connected Layer**: Maps the LSTM outputs to the vocabulary size for prediction.\n",
    "\n",
    "### Initialization:\n",
    "- Weights are initialized using uniform distributions for both the embedding and LSTM layers.\n",
    "- The `init_weights` method customizes weight initialization.\n",
    "\n",
    "### Forward Pass:\n",
    "- The forward method processes the input sequence (`src`) through the embedding, LSTM, and dropout layers, followed by a final fully connected layer to predict the next token in the sequence.\n",
    "\n",
    "### Hidden State Management:\n",
    "- The `init_hidden` method initializes the hidden state and cell state for LSTM layers.\n",
    "- The `detach_hidden` method detaches the hidden states from the computation graph to prevent gradient computation for subsequent batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,   \n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) #harry potter is\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction =self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training \n",
    "\n",
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 512                # 400 in the paper\n",
    "hid_dim = 512               # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65             \n",
    "lr = 1e-3                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 36,865,146 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, seq len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1188.215\n",
      "\tValid Perplexity: 553.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 506.114\n",
      "\tValid Perplexity: 304.066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 337.084\n",
      "\tValid Perplexity: 209.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 264.895\n",
      "\tValid Perplexity: 167.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 226.492\n",
      "\tValid Perplexity: 142.989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 201.566\n",
      "\tValid Perplexity: 126.115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 183.928\n",
      "\tValid Perplexity: 114.331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 170.931\n",
      "\tValid Perplexity: 105.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 160.634\n",
      "\tValid Perplexity: 98.228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 152.442\n",
      "\tValid Perplexity: 92.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 145.823\n",
      "\tValid Perplexity: 87.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 140.106\n",
      "\tValid Perplexity: 83.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 135.322\n",
      "\tValid Perplexity: 80.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 131.229\n",
      "\tValid Perplexity: 77.121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 127.613\n",
      "\tValid Perplexity: 74.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 124.378\n",
      "\tValid Perplexity: 72.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 121.695\n",
      "\tValid Perplexity: 70.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 119.119\n",
      "\tValid Perplexity: 68.329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 116.819\n",
      "\tValid Perplexity: 66.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 114.826\n",
      "\tValid Perplexity: 65.364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 112.847\n",
      "\tValid Perplexity: 63.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 111.112\n",
      "\tValid Perplexity: 62.610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 109.650\n",
      "\tValid Perplexity: 61.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 108.175\n",
      "\tValid Perplexity: 60.251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 106.821\n",
      "\tValid Perplexity: 59.407\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion,  batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mir Ali\\AppData\\Local\\Temp\\ipykernel_11608\\29452559.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 89.354\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the modelâ€™s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, vocab, itos_vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    \n",
    "    # Use your custom tokenizer to tokenize the prompt\n",
    "    tokens = simple_tokenize([prompt])[0]  # Tokenize the prompt and get the first (only) sentence\n",
    "    indices = [vocab.get(t, vocab['<unk>']) for t in tokens]  # Convert tokens to indices (using <unk> for unknown tokens)\n",
    "    \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            # Convert the token indices to tensor and move to the device\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            # Get the probabilities for the last token in the sequence\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)\n",
    "            \n",
    "            # Sample the next token based on the probabilities\n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            # If the prediction is <unk>, sample again\n",
    "            while prediction == vocab.get('<unk>', -1): \n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            # If the prediction is <eos>, stop generating\n",
    "            if prediction == vocab.get('<eos>', -1):    \n",
    "                break\n",
    "\n",
    "            # Add the predicted token index to the list for the next iteration\n",
    "            indices.append(prediction)\n",
    "\n",
    "    # Decode the generated token indices back to tokens (words)\n",
    "    tokens = [itos_vocab[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.5:\n",
      "Climate change is affecting countries like the new\n",
      "\n",
      "Temperature 0.7:\n",
      "Climate change is affecting countries like the palm of the new local\n",
      "\n",
      "Temperature 0.75:\n",
      "Climate change is affecting countries like the Americans, the White House said yesterday.\n",
      "\n",
      "Temperature 0.8:\n",
      "Climate change is affecting countries like the Americans, the palm of the new local\n",
      "\n",
      "Temperature 1.0:\n",
      "Climate change is affecting countries like U.S. quot;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Climate change is affecting countries like '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "\n",
    "# Generate text with each temperature value\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, vocab, itos_vocab, device, seed)\n",
    "    print(f\"Temperature {temperature}:\\n{' '.join(generation)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.5:\n",
      "Athletes are preparing for the upcoming Olympic Games\n",
      "\n",
      "Temperature 0.7:\n",
      "Athletes are preparing for the upcoming U.S. Olympic Committee in Canada.\n",
      "\n",
      "Temperature 0.75:\n",
      "Athletes are preparing for the upcoming U.S. Olympic Committee in Canada.\n",
      "\n",
      "Temperature 0.8:\n",
      "Athletes are preparing for the upcoming U.S. Olympic title in Athens.\n",
      "\n",
      "Temperature 1.0:\n",
      "Athletes are preparing for the upcoming U.S. Open.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Athletes are preparing for the upcoming '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "\n",
    "# Generate text with each temperature value\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, vocab, itos_vocab, device, seed)\n",
    "    print(f\"Temperature {temperature}:\\n{' '.join(generation)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.5:\n",
      "The stock market is seeing an <unk> due to the new\n",
      "\n",
      "Temperature 0.7:\n",
      "The stock market is seeing an <unk> due to the impact of the global economy.\n",
      "\n",
      "Temperature 0.75:\n",
      "The stock market is seeing an <unk> due to the impact of the US oil prices because his growing dollar.\n",
      "\n",
      "Temperature 0.8:\n",
      "The stock market is seeing an <unk> due to the impact of the US oil prices because his growing dollar.\n",
      "\n",
      "Temperature 1.0:\n",
      "The stock market is seeing an <unk> due to U.S. winter picture prices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'The stock market is seeing an uptrend due to'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "\n",
    "# Generate text with each temperature value\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, vocab, itos_vocab, device, seed)\n",
    "    print(f\"Temperature {temperature}:\\n{' '.join(generation)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sci/Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.5:\n",
      "Scientists have made a breakthrough in the evolution of the new species of humans, according to a study released yesterday.\n",
      "\n",
      "Temperature 0.7:\n",
      "Scientists have made a breakthrough in the evolution of the new\n",
      "\n",
      "Temperature 0.75:\n",
      "Scientists have made a breakthrough in the evolution of the planet that works on his\n",
      "\n",
      "Temperature 0.8:\n",
      "Scientists have made a breakthrough in the evolution of the planet that works on his side and\n",
      "\n",
      "Temperature 1.0:\n",
      "Scientists have made a breakthrough in the evolution of palm bodies from new spacecraft, researchers said.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Scientists have made a breakthrough in '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "\n",
    "# Generate text with each temperature value\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, vocab, itos_vocab, device, seed)\n",
    "    print(f\"Temperature {temperature}:\\n{' '.join(generation)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to lstm_model.pth\n",
      "Vocabulary saved to vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = \"lstm_model.pth\"\n",
    "vocab_file = \"vocab.pkl\"\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), model_file)\n",
    "print(f\"Model saved to {model_file}\")\n",
    "\n",
    "# Save the vocabulary\n",
    "with open(vocab_file, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(f\"Vocabulary saved to {vocab_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
