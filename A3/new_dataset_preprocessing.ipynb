{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "import datasets\n",
    "import gc\n",
    "import spacy \n",
    "import nltk\n",
    "\n",
    "import stanza\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcda2ae1b7b54d22a6914fc2b3fe0f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 21:50:12 INFO: Downloaded file to C:\\Users\\Mir Ali\\stanza_resources\\resources.json\n",
      "2025-02-02 21:50:12 INFO: Downloading default packages for language: sd (Sindhi) ...\n",
      "2025-02-02 21:50:13 INFO: File exists: C:\\Users\\Mir Ali\\stanza_resources\\sd\\default.zip\n",
      "2025-02-02 21:50:16 INFO: Finished downloading models and saved to C:\\Users\\Mir Ali\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "# Download Sindhi model\n",
    "stanza.download('sd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 21:50:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c162c46f544525aa21ffff149a65d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 21:50:16 INFO: Downloaded file to C:\\Users\\Mir Ali\\stanza_resources\\resources.json\n",
      "2025-02-02 21:50:16 INFO: Loading these models for language: sd (Sindhi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | isra    |\n",
      "=======================\n",
      "\n",
      "2025-02-02 21:50:16 INFO: Using device: cuda\n",
      "2025-02-02 21:50:16 INFO: Loading: tokenize\n",
      "2025-02-02 21:50:18 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Load the Sindhi pipeline with tokenizer\n",
    "nlp = stanza.Pipeline(lang='sd', processors='tokenize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: your safety or for the function of the appliance.\n",
      "Sindhi: Ù‡Ù† Ø¬ÙŠ Ù¾Ù†Ù‡Ù†Ø¬ÙŠ ÙÙ†ÚªØ´Ù† Ù„Ø§Ø¡Ù ØŒ ÙŠØ§ Ù‡Úª Ù…Ù†Ø§Ø³Ø¨ ÙÙ†ÚªØ´Ù† Ù„Ø§Ø¡Ù.\n",
      "\n",
      "English: â€¡Coded as 1 (female) or 2 (male).\n",
      "Sindhi: Ø·Ø±Ø­ 1 (Ø§Ù„Ù…Ø§Ø³), Ø·Ø±Ø­ 2 (Ø·Ù„Ø§)\n",
      "\n",
      "English: ãƒ‰ã‚­ãƒ‰ã‚­: day after day &\n",
      "Sindhi: Ø³Ø§Ù†Ú†Ùˆ:Ø¹Ù…Ø± Ø³Ø§Ù„Ù† Û½ ÚÙŠÙ†Ù‡Ù† Û¾/ÚÙŠÙ†Ù‡Ù†/Ø¯Ø³ØªØ§ÙˆÙŠØ²\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the English and Sindhi sentences\n",
    "with open(\"corpus/NLLB.en-sd.en\", \"r\", encoding=\"utf-8\") as f:\n",
    "    english_sentences = f.readlines()\n",
    "\n",
    "with open(\"corpus/NLLB.en-sd.sd\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sindhi_sentences = f.readlines()\n",
    "\n",
    "# Ensure both files have the same number of sentences\n",
    "assert len(english_sentences) == len(sindhi_sentences), \"Mismatch in the number of sentences\"\n",
    "\n",
    "# Create a list of sentence pairs\n",
    "parallel_corpus = list(zip(english_sentences, sindhi_sentences))\n",
    "\n",
    "# Example: Print the last 3 sentence pairs\n",
    "for en, sd in parallel_corpus[-3:]:\n",
    "    print(f\"English: {en.strip()}\")\n",
    "    print(f\"Sindhi: {sd.strip()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the alignment scores\n",
    "with open(\"corpus/NLLB.en-sd.scores\", \"r\", encoding=\"utf-8\") as f:\n",
    "    scores = [float(line.strip()) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile threshold: 1.100261206\n",
      "Filtered corpus size: 267741\n"
     ]
    }
   ],
   "source": [
    "# Calculate the 90th percentile score\n",
    "threshold = np.percentile(scores, 97)\n",
    "print(\"97th percentile threshold:\", threshold)\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_corpus = [(en, sd) for en, sd, score in zip(english_sentences, sindhi_sentences, scores) if score > threshold]\n",
    "print(f\"Filtered corpus size: {len(filtered_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered corpus to new files\n",
    "with open(\"filtered_en.txt\", \"w\", encoding=\"utf-8\") as f_en, open(\"filtered_sd.txt\", \"w\", encoding=\"utf-8\") as f_sd:\n",
    "    for en, sd in filtered_corpus:\n",
    "        f_en.write(en)\n",
    "        f_sd.write(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø²Ù†Ø¯Ú¯ÙŠ', 'Ù‡Úª', 'Ø³ÙØ±', 'Ø¢Ù‡ÙŠ']\n",
      "['Ø¢Ø¦ÙˆÙ†', 'Ø§Ø³ÚªÙˆÙ„', 'ÙˆÚƒØ§Ù†', 'Ù¿Ùˆ', '.']\n"
     ]
    }
   ],
   "source": [
    "def stanza_sindhi_tokenizer(text):\n",
    "    \"\"\"Tokenizes Sindhi text using Stanza.\"\"\"\n",
    "    doc = nlp(text)  # Process text using Stanza\n",
    "    tokens = [word.text for sentence in doc.sentences for word in sentence.words]  # Extract tokens\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "text1 = \"Ø²Ù†Ø¯Ú¯ÙŠ Ù‡Úª Ø³ÙØ± Ø¢Ù‡ÙŠ\"\n",
    "tokens1 = stanza_sindhi_tokenizer(text1)\n",
    "\n",
    "text2 = \"Ø¢Ø¦ÙˆÙ† Ø§Ø³ÚªÙˆÙ„ ÙˆÚƒØ§Ù† Ù¿Ùˆ.\"\n",
    "tokens2 = stanza_sindhi_tokenizer(text2)\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized English: ['This', 'is', 'a', 'test', '.']\n",
      "Tokenized Sindhi: ['Ù‡ÙŠ', 'Ù‡Úª', 'Ù½ÙŠØ³Ù½', 'Ø¢Ù‡ÙŠ', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load language models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")  # English\n",
    "\n",
    "# Tokenize a sentence\n",
    "english_sentence = \"This is a test.\"\n",
    "sindhi_sentence = \"Ù‡ÙŠ Ù‡Úª Ù½ÙŠØ³Ù½ Ø¢Ù‡ÙŠ.\"\n",
    "\n",
    "# Tokenize English using SpaCy\n",
    "doc_en = nlp_en(english_sentence)\n",
    "tokenized_en = [token.text for token in doc_en]\n",
    "\n",
    "# Tokenize Sindhi using Stanza\n",
    "tokenized_sd =stanza_sindhi_tokenizer(sindhi_sentence)\n",
    "\n",
    "print(\"Tokenized English:\", tokenized_en)\n",
    "print(\"Tokenized Sindhi:\", tokenized_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 267741\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Convert the filtered corpus into a Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    \"translation\": [\n",
    "        {\"en\": en.strip(), \"sd\": sd.strip()} for en, sd in filtered_corpus\n",
    "    ]\n",
    "})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 254353\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 6694\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 6694\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.05)  # 5% for the test set\n",
    "\n",
    "# Further split the test set into validation and test sets (e.g., 50% each)\n",
    "validation_test_split = train_test_split['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# Combine all splits into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_test_split['train'],\n",
    "    \"validation\": validation_test_split['train'],\n",
    "    \"test\": validation_test_split['test']\n",
    "})\n",
    "# Print the resulting DatasetDict\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b64249fa3e47838ba2d0eee49962d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/254353 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3649b19a35a9469186d1517bbc92b03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2a1d6b12bb4a379d897085ca330a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee82c6c9aacf40edae42f1f6761f3a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9783fbc8a1304dcbaa63de1fc46c2449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6dbf54594b432e84303e17811bbdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658b55dbfb4d48f7952ab47d5640b5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0127c59021fc40819f7d9a0dac63dbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ef35d84fa840f2899b8915565c7ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing necessary module\n",
    "from numpy.random import default_rng\n",
    "\n",
    "# Initializing a random number generator with a specified seed\n",
    "rng = default_rng(seed=SEED)\n",
    "\n",
    "# Selecting a random sample of indices from the training dataset\n",
    "selected_size = 500\n",
    "select_index = rng.choice(len(dataset['train']), size= selected_size, replace=False)\n",
    "\n",
    "# Filtering the training dataset based on the selected indices\n",
    "# This ensures that only the randomly selected subset of data is retained\n",
    "dataset['train'] = dataset['train'].filter(lambda data, index: index in select_index, with_indices=True)\n",
    "\n",
    "val_test_size = 100 \n",
    "select_index_val = rng.choice(len(dataset['validation']), size=val_test_size, replace=False)\n",
    "select_index_test = rng.choice(len(dataset['test']), size=val_test_size, replace=False)\n",
    "\n",
    "dataset['validation'] = dataset['validation'].filter(lambda data, index: index in select_index_val, with_indices=True)\n",
    "dataset['test'] = dataset['test'].filter(lambda data, index: index in select_index_test, with_indices=True)\n",
    "\n",
    "\n",
    "# Define a lambda function to extract the translation column for a specific language\n",
    "get_new_col = lambda data, lang: {lang: data['translation'][lang]}\n",
    "\n",
    "# Map the lambda function to create a new column for Sindhi translation\n",
    "dataset = dataset.map(get_new_col, fn_kwargs={'lang': \"sd\"})\n",
    "\n",
    "# Map the lambda function to create a new column for English translation and remove the original translation column\n",
    "dataset = dataset.map(get_new_col, remove_columns=['translation'], fn_kwargs={'lang': \"en\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(dataset['train'])))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'sd'\n",
    "\n",
    "## Importing of tokenizer libraries \n",
    "token_transform[\"en\"] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "token_transform[\"sd\"] = stanza_sindhi_tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence (English):  \"It just takes time to get everything where it needs to be.\n",
      "Tokenization (English):  ['\"', 'It', 'just', 'takes', 'time', 'to', 'get', 'everything', 'where', 'it', 'needs', 'to', 'be', '.']\n",
      "Sentence (Sindhi):  \"Ú‡Ø§ÚªØ§Ú» ØªÙ‡ Ù‡Ù† ÙˆÙ‚Øª Ù„ØªØ§ Ø¬Ùˆ Ù‡ØªÙŠ Ù‡ÙØ¬Ú» ØªÙ…Ø§Ù… Ø¶Ø±ÙˆØ±ÙŠ Ø¢Ù‡ÙŠ.\n",
      "Tokenization (Sindhi):  ['\"Ú‡Ø§ÚªØ§Ú»', 'ØªÙ‡', 'Ù‡Ù†', 'ÙˆÙ‚Øª', 'Ù„ØªØ§', 'Ø¬Ùˆ', 'Ù‡ØªÙŠ', 'Ù‡ÙØ¬Ú»', 'ØªÙ…Ø§Ù…', 'Ø¶Ø±ÙˆØ±ÙŠ', 'Ø¢Ù‡ÙŠ', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example sentence from the dataset\n",
    "sindhi_example = dataset['train']['sd'][111]\n",
    "sentence_english = dataset['train']['en'][111]\n",
    "\n",
    "print(\"Sentence (English): \", sentence_english)\n",
    "print(\"Tokenization (English): \", token_transform['en'](sentence_english)) \n",
    "\n",
    "print(\"Sentence (Sindhi): \", sindhi_example)\n",
    "print(\"Tokenization (Sindhi): \", token_transform['sd'](sindhi_example))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data, language):\n",
    "    language_index = {'sd': 0, 'en': 1}  # Update based on your dataset structure\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language])  # Access 'sd' or 'en' based on the `language` paramete\n",
    "        \n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Build vocab for each language ('sd' and 'en')\n",
    "for ln in ['sd', 'en']:\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(dataset['train'], ln),\n",
    "        min_freq=2,  # Set minimum frequency to avoid treating infrequent tokens as UNK\n",
    "        specials=special_symbols,\n",
    "        special_first=True\n",
    "    )\n",
    "\n",
    "# Set UNK_IDX as the default index\n",
    "for ln in ['sd', 'en']:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sd': 'Ø¬ÚÙ‡Ù† Ø¬ÚÙ‡Ù† Ø¨Ù‡ Ù…ÙÙ„Ùˆ Ù¿Ø§ ÚØ¦ÙŠ Ù¿Ø§ Ø²Ø®Ù… ÙˆÚƒÙˆØŒ', 'en': 'Whenever you suffer an injury.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])  # For first sample in training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ After Saving & Reloading:\n",
      "English vocab size: 528\n",
      "Sindhi vocab size: 679\n",
      "âœ… Vocabulary is correctly saved and reloaded!\n"
     ]
    }
   ],
   "source": [
    "torch.save(vocab_transform, 'vocab.pth')\n",
    "\n",
    "# Verify immediately after saving\n",
    "loaded_vocab_check = torch.load('vocab.pth')\n",
    "\n",
    "print(\"ğŸ”¹ After Saving & Reloading:\")\n",
    "print(\"English vocab size:\", len(loaded_vocab_check['en']))\n",
    "print(\"Sindhi vocab size:\", len(loaded_vocab_check['sd']))\n",
    "\n",
    "assert len(loaded_vocab_check['en']) == len(vocab_transform['en']), \"Mismatch after saving!\"\n",
    "assert len(loaded_vocab_check['sd']) == len(vocab_transform['sd']), \"Mismatch after saving!\"\n",
    "print(\"âœ… Vocabulary is correctly saved and reloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary keys (languages): dict_keys(['sd', 'en'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary keys (languages):\", vocab_transform.keys())  # Should be ['en', 'sd'] or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 528\n",
      "Sindhi Vocabulary Size: 679\n"
     ]
    }
   ],
   "source": [
    "print(\"English Vocabulary Size:\", len(vocab_transform['en']))\n",
    "print(\"Sindhi Vocabulary Size:\", len(vocab_transform['sd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sd', 'en'])\n"
     ]
    }
   ],
   "source": [
    "print(vocab_transform.keys())  # Should print dict_keys(['en', 'sd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(vocab_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "370\n"
     ]
    }
   ],
   "source": [
    "print(vocab_transform[TRG_LANGUAGE]['Ø¢Ø¦ÙˆÙ†'])\n",
    "print(vocab_transform[TRG_LANGUAGE]['Ù¾Ú»'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'world'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 111, for example\n",
    "mapping[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sd', 'en'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sd', 'en'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sd', 'en'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size for data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(dataset['validation'],  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(dataset['test'],  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, de in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 3])\n",
      "Sindhi shape:  torch.Size([64, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Sindhi shape: \", sd.shape)   # (batch_size, seq len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
