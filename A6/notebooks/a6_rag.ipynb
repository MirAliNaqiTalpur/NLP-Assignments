{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "from langchain.document_loaders import PyMuPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Source Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Find all relevant sources about yourself\n",
    "\n",
    "# List your personal documents\n",
    "personal_docs = [\n",
    "    \"../data/linkedin_profile.pdf\",\n",
    "    \"../data/personal_bio.txt\",\n",
    "    \"../data/AIT_SIS_personal_info.txt\"\n",
    "]\n",
    "\n",
    "# Load documents\n",
    "documents = []\n",
    "for doc_path in personal_docs:\n",
    "    try:\n",
    "        if doc_path.endswith('.pdf'):\n",
    "            loader = PyMuPDFLoader(doc_path)\n",
    "        elif doc_path.endswith('.txt'):\n",
    "            loader = TextLoader(doc_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {doc_path}\")\n",
    "            continue\n",
    "        docs = loader.load()\n",
    "        # Add source metadata to each document\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = os.path.basename(doc_path)\n",
    "        documents.extend(docs)\n",
    "        print(f\"Loaded: {doc_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {doc_path}: {e}\")\n",
    "\n",
    "print(f\"Total documents loaded: {len(documents)}\")\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "doc_chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(doc_chunks)} document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = FAISS.from_documents(doc_chunks, embedding_model)\n",
    "\n",
    "# Save vectorstore locally\n",
    "vectorstore.save_local(\"vectorstore\")\n",
    "print(\"Vector store saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful assistant that answers questions about Mir Ali Use the following context and chat history to provide a gentle and informative response. If the context doesn't provide the answer, politely say you don't have enough information.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Chat History: {chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    print(\"Error: GROQ_API_KEY environment variable not set.\")\n",
    "    exit()\n",
    "\n",
    "groq_llm = ChatGroq(\n",
    "    api_key=groq_api_key,\n",
    "    model_name=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up conversation memory\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=groq_llm,\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List retriever and generator models\n",
    "print(\"Retriever Model: FAISS with HuggingFace embeddings (sentence-transformers/all-MiniLM-L6-v2)\")\n",
    "print(\"Generator Model: Groq's llama-3.3-70b-versatile\")\n",
    "\n",
    "# 5) Analyze potential issues\n",
    "print(\"\"\"\n",
    "Analysis of Issues:\n",
    "- Retriever: FAISS may retrieve irrelevant chunks if embeddings fail to capture semantic meaning accurately. This could happen with ambiguous questions or insufficient document detail.\n",
    "- Generator: Groq's Llama3-70b might generate plausible but incorrect answers (hallucination) if retrieved context is incomplete.\n",
    "- Mitigation: Ensure documents are comprehensive, adjust chunk size/overlap, or refine the prompt to prioritize context adherence.\n",
    "\"\"\")\n",
    "\n",
    "# Task 3: Chatbot Development\n",
    "\n",
    "# Define the 10 required questions\n",
    "questions = [\n",
    "    \"How old are you?\",\n",
    "    \"What is your highest level of education?\",\n",
    "    \"What major or field of study did you pursue during your education?\",\n",
    "    \"How many years of work experience do you have?\",\n",
    "    \"What type of work or industry have you been involved in?\",\n",
    "    \"Can you describe your current role or job responsibilities?\",\n",
    "    \"What are your core beliefs regarding the role of technology in shaping society?\",\n",
    "    \"How do you think cultural values should influence technological advancements?\",\n",
    "    \"As a master's student, what is the most challenging aspect of your studies so far?\",\n",
    "    \"What specific research interests or academic goals do you hope to achieve during your time as a master's student?\"\n",
    "]\n",
    "\n",
    "# Generate answers and store in JSON format\n",
    "results = []\n",
    "for i, question in enumerate(questions, start=1):\n",
    "    response = qa_chain({\"question\": question})\n",
    "    answer = response[\"answer\"]\n",
    "    results.append({\n",
    "        \"question_number\": i,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "    # Print the question number, question, and answer\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(\"answers.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Answers saved to 'answers.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Analysis and Problem Solving\n",
    "\n",
    "### 1) List of Retriever and Generator Models\n",
    "- **Retriever Model:** FAISS with HuggingFace embeddings (sentence-transformers/all-MiniLM-L6-v2)\n",
    "- **Generator Model:** Groq's llama-3.3-70b-versatile\n",
    "\n",
    "### 2) Analysis of Issues\n",
    "\n",
    "**Retriever Model (FAISS with all-MiniLM-L6-v2 embeddings):**\n",
    "- Embedding Quality: The all-MiniLM-L6-v2 model is efficient but has limitations in capturing nuanced semantics compared to larger models\n",
    "- Chunk Size Impact: Our 1000-character chunks with 200-character overlap may split contextual information across chunks\n",
    "- Document Specificity: Personal documents may contain technical jargon or abbreviated information that embedding models might not accurately represent\n",
    "- Retrieval K-value: Using k=3 might miss relevant information if semantically similar but irrelevant chunks score higher\n",
    "\n",
    "**Generator Model (Groq's llama-3.3-70b-versatile):**\n",
    "- Hallucination Risk: The model may generate plausible-sounding but incorrect information when context is incomplete\n",
    "- Context Window Limitations: If the combined retrieved chunks exceed the model's context window, information may be truncated\n",
    "- Prompt Sensitivity: The generator's responses can vary significantly based on prompt wording and structure\n",
    "- Personality Alignment: The model may default to generic responses for personal questions if retrieved context lacks specific details\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "- Experiment with different chunk sizes (500-1500 characters) and overlaps (100-300 characters) to find optimal settings\n",
    "- Include more diverse personal documents to ensure comprehensive coverage of potential questions\n",
    "- Consider fine-tuning the retriever parameters, such as adjusting the similarity threshold or increasing k for more context\n",
    "- Implement answer validation by cross-referencing responses with known facts about yourself\n",
    "- Enhance the prompt to explicitly instruct the model to acknowledge uncertainty rather than generating potentially incorrect details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
